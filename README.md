

# 导言



> 我认为思想是运动的，而论证是驱动思想到某个方向的动力。
> 
> 
>  ——约翰·克雷格（John Craig, 1699）


我们在上一篇博客[《概率论沉思录：初等抽样论》](https://github.com)中介绍了传统的抽样理论。其中，我们导出了几种经典的抽样分布，也即给定关于所观察现象的假设\\(H\\)，数据\\(D\\)的概率分布\\(p(D\\mid H)\\)。在上一篇博客中提到的伯努利坛子模型中，假设\\(H\\)即坛子的内容，数据\\(D\\)即重复抽球所生成的红球和白球序列。但正如我们我们在上一篇博客的末尾所述，几乎所有实际的科学推断问题都处在相反的使用场景：我们已知数据\\(D\\)，希望确定假设\\(H\\)。更一般地说，已知数据\\(D\\)，如何求概率分布\\(p(H\_1\\mid D), p(H\_2\\mid D), \\cdots\\)，以指出给定假设\\(\\{H\_1, H\_2, \\cdots\\}\\)中哪一个成立？


例如，我们的假设可能是对生成数据的物理机制的各种推断。但是从根本上讲，物理因果关系不是问题的必要组成部分，重要的只是假设和数据之间有某种逻辑关系。我们将这类问题称为**假设检验（hypothesis testing）**。



> **注** 本书\[1]\[2]采用贝叶斯派的视角，参数估计的过程实际上就是在进行假设检验了。因此，接下来讲的假设检验将与频率派的假设检验不太一样。事实上，贝叶斯派的假设检验不需要概率之外的特定工具（ad hoc devices），而频率派需要。


# 1 科学推断的基本原理


首先，我们引入先验概率的概念。除了与当前问题有关的新信息或数据\\(D\\)之外，我们用\\(X\\)来表示机器人几乎总是会拥有的其它信息。这至少包括它从离开工厂到收到当前问题为止的所有过去经验。对于机器人来说，所有概率至少要以\\(X\\)为条件。我们称仅以\\(X\\)为条件的概率\\(P(A\\mid X)\\)为**先验概率（prior probability）**。需要注意的是，“先验”一词并不一定意味着时间上更早，这种区别纯粹是逻辑上的。根据定义，除了当前问题的直接数据\\(D\\)之外的任何其它信息都是“先验信息”。



> **注** 还需要指出的是，伊曼努尔·康德（Immanuel Kant）引入a\-priori\[3]一词来表示可以独立于经验而知道真假的命题，而我们这里使用的“先验信息”不表示这种意思。\\(X\\)只简单地表示机器人拥有的我们所称“数据”之外的其它信息。


引入先验概率后，再加上我们在博客[《概率论沉思录：定量规则》](https://github.com)中提到的乘法规则，我们就可以着手解决假设检验问题了。现做如下命题定义：


* \\(X\\)：先验信息。
* \\(H\\)：待检验的假设。
* \\(D\\)：数据。


根据乘法规则，我们有：


\\\[P(DH\\mid X) \= P(D\\mid HX)P(H\\mid X) \= P(H\\mid DX)P(D\\mid X)
\\]在上一篇博客[《概率论沉思录：初等抽样论》](https://github.com)中，我们并不需要特别注意先验信息\\(X\\)，因为所有概率都以\\(H\\)为条件，所以我们可以隐含地假设，定义问题的一般先验信息已经包含在\\(H\\)中。但是现在，所求的这些概率不再至少以\\(H\\)为条件，而是至少以\\(X\\)为条件，因此需要为它们使用不同的符号。


考虑上式的最后一个等式，进行移项后可以将\\(P(H\\mid DX)\\)表示为\\(P(H\\mid X)\\)乘上一个对\\(H\\)先验概率的调整因子：


\\\[\\underbrace{P(H\\mid DX)}\_{H\\text{的后验概率}} \= \\underbrace{P(H\\mid X)}\_{H\\text{的先验概率}}\\boxed{\\frac{P(D\\mid HX)}{P(D\\mid X)}}\_{调整因子} \\tag{1}
\\]关于上述等式的各项，我们做以下的名词约定：


* \\(P(H\\mid DX)\\)：称为**后验概率（posterior probability）**。同样需要注意的是，这仅意味着“在逻辑上处在特定推理链的后面”，而不一定“时间上更晚”。一个人的先验概率可能是另一个人的后验概率。实际上只有一种概率，我们使用不同的名称仅指组织计算的特定方式。
* \\(P(D\\mid HX)\\)：称为**似然（likelihood）**，记作\\(L(H)\\)。可以看出\\(P(D\\mid HX)\\)是我们在上一篇博客[《概率论沉思录：初等抽样论》](https://github.com)中介绍的抽样分布，它在固定\\(H\\)时依赖于\\(D\\)。但是在这篇博客中，我们将根据不同的假设\\(\\{H, H^{\\prime}, \\cdots\\}\\)考察固定的数据集\\(D\\)，当固定\\(D\\)考察\\(P(D\\mid HX)\\)对\\(H\\)的依赖时，我们称其为“似然”。似然\\(L(H)\\)本身并不是\\(H\\)的概率。它是一个无量纲的数值函数。当与\\(H\\)的先验概率和归一化因子相乘时，它可以成为概率。
* \\(P(D\\mid X)\\)：称为**归一化因子**。注意，很多文献和教材将这里的归一化因子称为“证据”，但“证据”在本书中已经被用于定义其它的东西，故在此说明一下。


对于许多科学推断问题，式\\((1\)\\)指出了需要计算哪些概率才能判断我们的全部证据证明了哪些结论是合情的。如果\\(P(H\\mid DX)\\)非常接近1（或0），那么我们可以得出结论：\\(H\\)非常可能为真（或假），并采取相应的行动。但是，如果\\(P(H\\mid DX)\\)距\\(1/2\\)不远，则机器人会警告我们可用的证据不足以证明任何可靠的结论，我们需要获得更多更好的证据。


# 2 二元假设检验


最简单的假设检验问题只有两个假设要检验，并且只有两种可能的结果。首先，我们使式\\((1\)\\)变成这种二元情形。它给出了\\(H\\)为真的概率；对于\\(H\\)为假的概率，我们同样可以写出


\\\[P(\\overline{H}\\mid DX) \= P(\\overline{H}\\mid X)\\frac{P(D\\mid \\overline{H}X)}{P(D\\mid X)}
\\]取两个等式的比值，得到


\\\[\\frac{P(H\\mid DX)}{P(\\overline{H}\\mid DX)} \= \\frac{P(H\\mid X)}{P(\\overline{H}\\mid X)}\\frac{P(D\\mid HX)}{P(D\\mid \\overline{H}X)}
\\]这里我们拥有的量，即\\(H\\)为真的概率与它为假的概率之比，我们称其为命题\\(H\\)的 **“几率”（odds）**。



> **注** odds在赌博的场景中一般翻译成“赔率”，在本书中它只是用作\\(p/(1 \- p)\\)的代名词，是概率的单调函数。本书中都翻译成几率。


定义\\(O(H\\mid DX)\\equiv \\frac{P(H\\mid DX)}{P(\\overline{H}\\mid DX)}\\)，我们可以将上式写为：


\\\[O(H\\mid DX) \= O(H\\mid X)\\frac{P(D\\mid HX)}{P(D\\mid \\overline{H}X)}
\\]可见\\(H\\)的后验几率等于\\(H\\)的先验几率乘以一个叫做似然比的无量纲因子。


在许多应用中，取几率的对数会更方便，因为我们可以累加各项。我们定义一个新函数，称为给定\\(D\\)和\\(X\\)时\\(H\\)的**证据（evidence）**：


\\\[e(H\\mid DX) \\equiv 10 \\log\_{10}O(H\\mid DX)
\\]它仍然是概率的单调函数。通过使用底数\\(10\\)并将因子\\(10\\)放在前面，我们现在以**分贝（decibels，以下简写为\\(\\text{dB}\\)）** 为单位来衡量证据。在给定\\(D\\)的情况下，\\(H\\)的证据等于\\(H\\)的先验证据加上通过计算下式最后一项中的对数似然所得到的\\(\\text{dB}\\)数量：


\\\[e(H\\mid DX) \= e(H\\mid X) \+ 10\\log\_{10}\\left\[\\frac{P(D\\mid HX)}{P(D\\mid \\overline{H}X)}\\right]
\\]现在假设这个新信息\\(D\\)实际上包含几个不同的命题：\\(D \= D\_1 D\_2 D\_3 \\cdots\\)。那么，应用乘法规则有：\\(\\frac{P(D\\mid HX)}{P(D\\mid \\overline{H}X)} \= \\frac{P(D\_1\\mid HX)}{P(D\_1\\mid \\overline{H}X)}\\cdot \\frac{P(D\_2\\mid D\_1HX)}{P(D\_2\\mid D\_1\\overline{H}X)}\\cdot \\cdots\\)。但在许多情况下，获得\\(D\_2\\)的概率不受关于\\(D\_1\\)的知识的影响，即\\(P(D\_2\\mid D\_1HX)\=P(D\_2\\mid HX)\\)，也即机器人分配给\\(D\_1\\)和\\(D\_2\\)的概率是**独立（independent）** 的。再次重申：我们关注的是逻辑独立性，而不是物理的因果独立性。通常，随着机器人的知识状态（以\\(H\\)和\\(X\\)表示）发生变化，以它们为条件的概率可能会从相互独立的变为相互依赖的，反之亦然。但是事件的真实属性保持不变。


如果在给定\\(HX\\)的条件下，数据\\(D\_1, D\_2, D\_3, \\cdots\\)的概率是逻辑独立的，则似然比可以展开为


\\\[e(H\\mid DX) \= e(H\\mid X) \+ \\sum\_{i} 10\\log\_{10}\\left\[\\frac{P(D\_i\\mid HX)}{P(D\_i\\mid \\overline{H}X)}\\right] \\tag{2}
\\]其中的和式取遍我们获得的所有额外信息。


为了对这里的数值有直观的认识，我们可以将证据（\\(e\\)）、几率（\\(O\\)）和概率（\\(p\\)）构建成如下的表：





| 证据 (\\(e\\)) | 几率 (\\(O\\)) | 概率 \\((p)\\) |
| --- | --- | --- |
| \\(0\\) | \\(1:1\\) | \\(1/2\\) |
| \\(3\\) | \\(2:1\\) | \\(2/3\\) |
| \\(6\\) | \\(4:1\\) | \\(4/5\\) |
| \\(10\\) | \\(10:1\\) | \\(10/11\\) |
| \\(20\\) | \\(100:1\\) | \\(100/101\\) |
| \\(30\\) | \\(1000:1\\) | \\(0\.999\\) |
| \\(40\\) | \\(10000: 1\\) | \\(0\.9999\\) |
| \\(\-e\\) | \\(1/O\\) | \\(1 \- p\\) |



进一步绘制成如下所示的图：



![电影爱好者的评分情况示意图](https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2106514/o_dfe1a252.png)



从上面的图和表中我们可以明显地看出为什么以分贝（\\(\\text{dB}\\)）为单位给出证据非常有力。当概率接近\\(1\\)或\\(0\\)时，我们的直觉很差。对我们来说，\\(0\.999\\)和\\(0\.9999\\)的概率差别没多大意义，但是\\(30\\text{dB}\\)和\\(40\\text{dB}\\)的证据之间的差别确实对我们有明确意义。


现在让我们将式\\((2\)\\)应用于一个特定的工业质量问题中（尽管也可以将其表述为其它问题）。假设先验信息\\(X\\)如下：


* \\(X\\)：我们有11台自动机器，这些机器将其生产出的小部件输出到11个盒子中。该过程对应于小部件生产的早期阶段，因为有10台机器会生产1/6的坏部件。第11台机器更糟，会生产1/3的坏部件。每台机器输出的部件被分别放在一个未贴标签的盒子中，并存储在仓库中。


我们选择一个盒子并抽样检测其中的一些小部件，将它们分为“好”和“坏”。我们的目标是判断是否选择了那个糟糕机器对应的盒子，然后判断是要接受还是拒绝它。


我们把这项工作交给我们的机器人，看看它是如何工作的。首先，它必须找到待检验假设的先验证据。我们定义以下两个假设：


* \\(A\\)：选择了\\(1/3\\)的次品率的坏批次。
* \\(B\\)：选择了\\(1/6\\)的次品率的好批次。


先验信息\\(X\\)的定性部分告诉我们，只有两种可能性。因此，在\\(X\\)产生的逻辑背景下，两个命题是互否的关系：给定\\(X\\)，我们有\\(\\overline{A} \= B,\\quad \\overline{B}\=A\\)。


唯一的定量先验信息是有11台机器，我们不知道是哪台机器制造了我们选择的批次，因此根据无差别原则有\\(P(A\\mid X)\=1/11\\)，于是


\\\[e(A\\mid X) \= 10 \\log\_{10}\\frac{P(A\\mid X)}{P(\\overline{A}\\mid X)} \= 10\\log\_{10}\\frac{1/11}{10/11} \= \-10\\text{dB}
\\]（同理，我们有\\(e(B\\mid X) \= 10\\text{dB}\\)）


在此问题中，\\(X\\)与计算有关的唯一信息只是这些数值，即\\(\\pm 10 \\text{dB}\\)。因此，我们没必要说我们仅在谈论11台机器的问题。可能只有一台机器，而这里的先验信息是我们之前使用它的经验：使用该机器时，有多少概率遇到好批次/坏批次。在这里，重要的是好批次/坏批次的先验概率。


如果我们取出一个坏部件，将会增加这是坏批次的证据；如果我们取出一个好部件，将会减少这是坏批次的证据。我们设\\(N\\)为批次中的部件总数，我们依次抽取\\(n\\)个部件进行检测，且假设\\(N\\gg n\\)，也即我们连续进行\\(n\\)次有放回抽样，此时正如我们在上一篇博客[《概率论沉思录：初等抽样论》](https://github.com):[MeoMiao 萌喵加速](https://biqumo.org)中提到的，超几何分布的极限形式，即二项分布将适用。设我们检测的\\(n\\)个部件中，有\\(b\\)个坏部件和\\(g\\)个好部件，则我们可以得到这是坏批次的后验证据为


\\\[\\begin{aligned}
 e(A\\mid DX) \&\= e(A\\mid X) \+ \\sum\_{i\=1}^{b} 10\\log\_{10}\\left\[\\frac{P(\\text{坏}\\mid AX)}{P(\\text{坏}\\mid \\overline{A}X)}\\right] \+ \\sum\_{i\=1}^{g} 10\\log\_{10}\\left\[\\frac{P(\\text{好}\\mid AX)}{P(\\text{好}\\mid \\overline{A}X)}\\right]\\\\
 \& \= e(A\\mid X) \+ b \\cdot 10\\log\_{10}\\frac{1/3}{1/6} \+ g\\cdot 10\\log\_{10}\\frac{2/3}{5/6}\\\\
 \& \\approx e(A\\mid X) \+ 3b \-g
\\end{aligned} \\tag{3}
\\]可见，一旦我们使用对数，计算是多么简单。机器人的思想以一种非常简单直接的方式“朝某个方向被驱动”。假设我们抽样的样本有80%的小部件是坏的，我们可以将其可视化为如下所示的图：



![电影爱好者的评分情况示意图](https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2106514/o_41c7a9bb.png)



现在，我们拥有的只是选择了坏批次的假设的概率、几率或证据。最终，我们必须做一个决定：是接受它，还是拒绝它。这时我们该怎么办呢？当然，我们可以事先决定：如果假设\\(A\\)的概率达到一定的值，那么就判定\\(A\\)为真，如果它下降到某个值，那么就判定\\(A\\)为假。


概率论本身不会告诉我们做出决策的临界值在哪里。这必须基于价值判断：做出错误决定的后果是什么？进行进一步检测的代价是什么？这会将我们带入决策论领域，我们后面会进行讨论。目前比较明显的是犯第一类错误（接受坏批次）可能比犯另一类错误（拒绝好批次）的后果更为严重。这将对我们如何设置临界值产生明显的影响。


因此，我们可以给机器人一些指示，例如“如果\\(A\\)的证据大于\\(0\\text{dB}\\)，则拒绝该批次（它很可能是坏的而不是好的）。如果\\(A\\)的证据低至\\(\-13\\text{dB}\\)，则接受该批次（它至少有\\(95\\%\\)的概率是好的）。否则，请继续检测。”


上述方法是我们的机器人根据命题\\(A\\)的后验概率达到一定水平后选择拒绝它或接受它的方法，这个非常有用且强大的流程在统计文献中称为 **“序列推断（sequential inference）”**，该术语表明检测次数不是预先确定的，而是取决于我们发现的数据值的顺序。


# 3 多重假设检验


假定在刚刚讨论的序列检测过程中，我们测试了50个小部件，结果每个小部件都是坏的。根据式\\((3\)\\)，坏批次假设证据\\(e(A\\mid DX)\\)的最终结果是\\(140\\text{dB}\\)，这是\\(1\-10^{\-14}\\)的概率。但是，我们的常识会倾向于拒绝这一结论，我们会对这个批次中只有\\(1/3\\)是坏部件产生怀疑。


在当前的问题中，我们可以使机器人在看到“太多”坏部件时对\\(A\\)持怀疑态度，方法是额外提供一个指出这种可能性的假设。我们在假设\\(A\\)：我们有一个有\\(1/3\\)坏部件的盒子，假设\\(B\\)：我们有一个有\\(1/6\\)坏部件的盒子的基础之上，添加第三个假设\\(C\\)：制造小部件的机器完全出了问题，会生产\\(99\\%\\)的坏部件。


现在，我们必须调整先前的概率，以考虑这种新的可能性。但是我们不希望问题的性质发生重大改变。因此，我们让假设\\(C\\)的先验概率\\(P(C\\mid X)\\)非常低，为\\(10^{\-6}\\)（\\(\-60\\text{dB}\\)）。


我们定义以下三个假设：


* \\(A\\)：我们选择了有\\(1/3\\)坏部件的盒子。
* \\(B\\)：我们选择了有\\(1/6\\)坏部件的盒子。
* \\(C\\)：我们选择了有\\(99/100\\)坏部件的盒子。


这三个假设的初始概率依次为：\\(P(A\\mid X)\=\\frac{1}{11}(1 \- 10^{\-6}), P(B\\mid X)\=\\frac{10}{11}(1 \- 10^{\-6}), P(C\\mid X)\=10^{\-6}\\)。因子\\(1 \- 10^{\-6}\\)实际上可以忽略不计，于是我们有


\\\[e(A\\mid X) \= \-10\\text{dB},\\quad e(B\\mid X) \= 10\\text{dB},\\quad e(C\\mid X) \= \-60\\text{dB}
\\]设与数据有关的命题\\(D\\)是“我们抽样检测的\\(n\\)个部件中，每个都是坏部件”，则我们可以得到命题\\(C\\)的后验证据为


\\\[\\begin{aligned}
 e(C\\mid DX) \&\= e(C\\mid X) \+ 10\\log\_{10}\\left\[\\frac{P(D\\mid CX)}{P(D\\mid \\overline{C}X)}\\right]
\\end{aligned} \\tag{4}
\\]其中\\(P(D\\mid CX)\=(\\frac{99}{100})^{n}\\)（我们仍然假设盒子里的小部件总数\\(N\\)比被抽样检测的数量\\(n\\)大很多，因此这里近似为无放回抽样）。而对于\\(P(D\\mid \\overline{C}X)\\)，我们在计算的过程中将会用到两次乘法规则：


\\\[\\begin{aligned}
 P(D\\mid \\overline{C}X) \&\= \\frac{P(D\\mid X)P(\\overline{C}\\mid DX)}{P(\\overline{C}\\mid X)}\\\\
 \&\= \\frac{P(D\\mid X)\\left\[P(A\\mid DX) \+ P(B\\mid DX)\\right]}{P(A\\mid X) \+ P(B\\mid X)}\\\\
 \& \= \\frac{P(D\\mid X)\\left\[\\frac{P(D\\mid AX)P(A\\mid X)}{P(D\\mid X)} \+ \\frac{P(D\\mid BX)P(B\\mid X)}{P(D\\mid X)}\\right]}{P(A\\mid X) \+ P(B\\mid X)}\\\\
 \& \= \\frac{P(D\\mid AX)P(A\\mid X) \+ P(D\\mid BX)P(B\\mid X)}{P(A\\mid X) \+ P(B\\mid X)}\\\\
 \& \= \\frac{(\\frac{1}{3})^{n}(\\frac{1}{11}) \+ (\\frac{1}{6})^{n}(\\frac{10}{11})}{(\\frac{1}{11}) \+ (\\frac{10}{11})}(1\-10^{\-6}忽略不计)\\\\
 \& \= (\\frac{1}{11})(\\frac{1}{3})^{n} \+ (\\frac{10}{11})(\\frac{1}{6})^{n}
\\end{aligned}
\\]于是我们有


\\\[e(C\\mid DX) \= e(C\\mid X) \+ 10\\log\_{10}\\left\[\\frac{(\\frac{99}{100})^n}{(\\frac{1}{11})(\\frac{1}{3})^{n} \+ (\\frac{10}{11})(\\frac{1}{6})^{n}}\\right] \\tag{5}
\\]如果\\(n \> 5\\)，一个很好的近似是


\\\[e(C\\mid DX) \\approx \-49\.6 \+ 4\.73n,\\quad n \> 5
\\]如果\\(n \< 5\\)，一个很好的近似是


\\\[e(C\\mid DX) \\approx \-60 \+ 7\.73n,\\quad n \< 3
\\]与此同时，我们想知道假设\\(A\\)和\\(B\\)发生了什么。在测试了\\(n\\)个小部件并且证明了它们都是坏的之后，假设\\(A\\)和假设\\(B\\)的证据以及近似形式如下：


\\\[\\begin{aligned}
 e(A\\mid DX) \&\= e(A\\mid X) \+ 10\\log\_{10}\\left\[\\frac{(\\frac{1}{3})^n}{(\\frac{1}{6})^{n} \+ \\frac{11}{10}\\times 10^{\-6}(\\frac{99}{100})^{n}}\\right]\\\\
 \&\\approx \\left\\{\\begin{aligned}
 \-10 \+ 3n,\\quad n \< 7,\\\\
 \+49\.6 \- 4\.73n,\\quad n \> 8
 \\end{aligned}\\right.
\\end{aligned} \\tag{6}
\\]\\\[\\begin{aligned}
 e(B\\mid DX) \&\= e(B\\mid X) \+ 10\\log\_{10}\\left\[\\frac{(\\frac{1}{6})^n}{(\\frac{1}{3})^{n} \+ 11\\times 10^{\-6}(\\frac{99}{100})^{n}}\\right]\\\\
 \&\\approx \\left\\{\\begin{aligned}
 10 \- 3n,\\quad n \< 10,\\\\
 \+59\.6 \- 7\.73n,\\quad n \> 11
 \\end{aligned}\\right.
\\end{aligned} \\tag{7}
\\]假设\\(A\\)、\\(B\\)、\\(C\\)的证据随抽样次数的变化如下图所示：



![电影爱好者的评分情况示意图](https://images.cnblogs.com/cnblogs_com/blogs/538207/galleries/2106514/o_0879d567.png)



可以看到，曲线\\(A\\)和曲线\\(B\\)的初始直线部分代表我们在引入新假设\\(C\\)之前发现的解。新假设\\(C\\)在初始时会被暂时搁置， 它的影响直到\\(C\\)穿过\\(B\\)时才出现。从这一点往后，曲线\\(A\\)不再继续向上，而是转而向下。机器人确实已经学会了如何怀疑。但是，曲线\\(B\\)在这一点上并没有改变，它一直线性延伸到\\(A\\)和\\(C\\)具有相同合情性的位置。


对这种现象的解释是，上述的多重序列检测可以近似看作是交替进行的二元假设检验：最初\\(B\\)的合情性远高于\\(C\\)，我们实际上基本上是在针对\\(B\\)检验\\(A\\)，然后重现了式\\((3\)\\)的解。在积累了足够的证据后，\\(C\\)的合情性达到了与\\(B\\)相同的水平之后，基本上将是针对\\(C\\)而不是\\(B\\)检验\\(A\\)。


更一般地说，只要我们有一组离散的假设，则其中任何一个的合情性变化都将近似是针对单个备择假设——所有假设当中最合情的那个备择假设进行检验的结果。


# 4 连续概率分布函数


接下来，我们对上面的例子进行扩展。直截了当的是引入更多的“离散”假设。更有趣的是引入一系列连续的假设，例如


* \\(H\_f\\)：机器人以\\(f\\)的比例生产坏部件（\\(f\\)可以是\\(0\\leqslant f\\leqslant 1\\)中的任何数值）。


这样，与离散的先验分布不同，我们的机器人需要考虑\\(f\\)在区间\\((0\\leqslant f \\leqslant 1\)\\)中具有的连续分布，并将根据观察到的样本计算\\(f\\)取各种值的后验概率，由此可以做出各种决策。在继续我们对假设检验问题的讨论之前，我们先来讨论连续概率分布。


我们在博客[《概率论沉思录：定量规则》](https://github.com)中导出的推断规则仅针对离散命题（\\(A, B, \\cdots\\)）的有限集合情况得出，但我们在实践中可以将涉及连续假设的问题进行转换，然后用这些规则进行处理。假设\\(f\\)是我们感兴趣的任意连续实参数变量，则我们可以定义以下离散、互斥且完备的命题：


\\\[F^{\\prime} \\equiv (f \\leqslant q),\\quad F^{\\prime\\prime} \\equiv (f \> q)
\\]因此，我们的规则一定适用于它们。给定一些先验信息\\(X\\)，则\\(F^{\\prime}\\)的概率通常取决于\\(q\\)，从而定义


\\\[G(q) \\equiv p(F^{\\prime}\\mid X)
\\]它显然是单调增加的。接下来我们来看\\(f\\)位于指定区间（\\(a\_1 \< f \\leqslant a\_2\\)）的概率是多少。我们定义以下命题：


\\\[A \\equiv (f \\leqslant a\_1\),\\quad B \\equiv (f \\leqslant a\_2\),\\quad W \\equiv (a\_1 \< f \\leqslant a\_2\)
\\]则布尔代数关系为\\(B \= A \+ W\\)，由于\\(A\\)和\\(W\\)互斥，则加法规则可简化为\\(P(B\\mid X)\=P(A\\mid X) \+ P(W\\mid X)\\)。又因为\\(P(B\\mid X)\=G(a\_2\)\\)，\\(P(A\\mid X)\=G(a\_1\)\\)，所以我们有


\\\[P(a\_1 \< f \\leqslant a\_2\\mid X) \= P(W\\mid X) \= G(a\_2\) \- G(a\_1\)
\\]在当前情况下，\\(G(q)\\)是连续可微的，所以我们也可以写出


\\\[P(a\_1 \< f \\leqslant a\_2\\mid X) \= \\int\_{a\_1}^{a\_2} g(f)\\mathrm{d}f
\\]其中\\(g(f)\=G^{\\prime}(f)\\geqslant 0\\)是\\(G\\)的导数，通常称为**概率分布函数（probability distribution function）**，或给定\\(X\\)时\\(f\\)的**概率密度函数（probanility density function）**。我们此后使用缩写PDF来表示它，与上述两种英文名称均一致。它的积分\\(G(f)\\)可以称为\\(f\\)的**累积分布函数（cumulative distribution function）**。


# 5 检验无数假设


现在假定我们同时要检验无数个假设。我们可以使用分析的方法来使问题变得更简单。但是，之前我们采用的对数形式的公式就不太好用了，因此我们下面会回到式\\((1\)\\)中的原始概率形式：


\\\[P(A\\mid DX) \= P(A\\mid X)\\frac{P(D\\mid AX)}{P(D\\mid X)}
\\]现在让\\(A\\)代表假设“坏部件比例在\\((f, f \+ \\mathrm{d}f)\\)的范围内”，其先验PDF为：


\\\[P(A\\mid X) \= g(f\\mid X)\\mathrm{d}f
\\]这给出了坏部件比例在\\(\\mathrm{d}f\\)区间内的概率。令\\(D\\)表示迄今为止我们的实验结果：


* \\(D\\)：抽样检测\\(n\\)个小部件，其中有\\(b\\)个坏部件和\\(n\-b\\)个好部件。


那么\\(f\\)的后验PDF是


\\\[P(A\\mid DX) \= P(A\\mid X)\\frac{P(D\\mid AX)}{P(D\\mid X)}\=g(f\\mid DX)\\mathrm{d}f
\\]因此，先验PDF与后验PDF由


\\\[g(f\\mid DX) \= g(f\\mid X)\\frac{P(D\\mid AX)}{P(D\\mid X)}
\\]关联。分母是归一化常数。如果需要，通常可以要求后验PDF满足归一化条件\\(P(0\\leqslant f\\leqslant 1\\mid DX) \= \\int\_{0}^1g(f\\mid DX)\\mathrm{d}f \= 1\\Rightarrow\\int\_0^1g(f\\mid X)\\frac{P(D\\mid AX)}{P(D\\mid X)}\\mathrm{d}f\=1\\)，从而更简单地确定该分母：


\\\[P(D\\mid X) \= \\int\_0^1g(f\\mid X)P(D\\mid AX)\\mathrm{d}f
\\]我们有\\(\\mathrm{d}f\\rightarrow 0\\)时，\\(P(D\\mid AX)\\rightarrow P(D\\mid H\_fX)\\)（详细证明过程请参见原书）。考虑假设\\(H\_f\\)：机器人以\\(f\\)的比例生产坏部件，则在每次试验中取出坏部件的概率为\\(f\\)，取出好部件的概率为\\((1 \- f)\\)。现在，我们不仅有假设盒子里的小部件总数\\(N\\)比被抽样检测的数量\\(n\\)大很多，因此不同试验的概率在给定\\(f\\)时是逻辑独立的；而且\\(f\\)还是一个连续的数值。因此，类似我们在上一篇博客[《概率论沉思录：初等抽样论》](https://github.com)中推导二项分布那样，可以得到


\\\[P(D\\mid H\_fX) \= f^{b} (1 \- f)^{n \- b}
\\]（注意，这里与二项分布不同的是，实验数据\\(D\\)是有顺序的）


因此，我们的后验PDF就可以表示为


\\\[g(f\\mid DX) \= \\frac{f^{b} (1\-f)^{n\-b}g(f\\mid X)}{\\int\_0^1f^{b} (1 \- f)^{n \- b}g(f\\mid X)\\mathrm{d}f} \\tag{8}
\\]我们在这篇博客中介绍的二元假设检验检验、多重假设检验都做为特殊情况包含在了这个公式中。例如我们之前讨论的针对\\(A\\)、\\(B\\)、\\(C\\)三种假设的检验，其对应的先验PDF如下所示：


\\\[g(f\\mid X) \= \\underbrace{\\frac{1}{11}(1 \- 10^{\-6})}\_{\\text{假设}A\\text{的先验PDF}}\\delta(f \- \\frac{1}{3}) \+ \\underbrace{\\frac{10}{11}(1\-10^{\-6})}\_{\\text{假设}B\\text{的先验PDF}}\\delta(f\-\\frac{1}{6}) \+ \\underbrace{10^{\-6}}\_{\\text{假设}C\\text{的先验PDF}}\\delta (f \- 99/100\)
\\]这里的\\(\\delta\\)函数在除了0以外的点函数值都等于0，而在其整个定义域上的积分等于1。当\\(f\\)分别取值\\(\\frac{1}{6}, \\frac{1}{3}, \\frac{99}{100}\\)时，先验PDF分别为\\(\\frac{10}{11}(1 \- 10^{\-6}), \\frac{1}{11}(1 \- 10^{\-6}), 10^{\-6}\\)。


运用这里的后验PDF表达式来重新考虑我们之前提到的针对\\(A\\)、\\(B\\)、\\(C\\)三种假设的检验问题，我们考虑对单个假设\\(C\\)进行假设检验（\\(f\_A\=\\frac{1}{6}, f\_B\=\\frac{1}{3}, f\_C\=\\frac{99}{100}\\)），有


\\\[\\begin{aligned}
 P(C\\mid DX) \&\= \\frac{(\\frac{99}{100})^{n}10^{\-6}\\delta(0\)}{(\\frac{1}{3})^n\\frac{1}{11}(1 \- 10^{\-6})\\delta(0\) \+ (\\frac{1}{6})^{n}\\frac{10}{11}(1 \- 10^{\-6})\\delta(0\) \+ (\\frac{99}{100})^m10^{\-6}\\delta(0\)}\\\\
 \&\= \\frac{(\\frac{99}{100})^{n}10^{\-6}}{(\\frac{1}{3})^n\\frac{1}{11}(1 \- 10^{\-6}) \+ (\\frac{1}{6})^{n}\\frac{10}{11}(1 \- 10^{\-6}) \+ (\\frac{99}{100})^n10^{\-6}}
\\end{aligned}
\\]对比我们之前得到的\\(e(C\\mid DX)\\)：


\\\[\\begin{aligned}
 e(C\\mid DX) \&\= e(C\\mid X) \+ 10\\log\_{10}\\left\[\\frac{(\\frac{99}{100})^n}{(\\frac{1}{11})(\\frac{1}{3})^{n} \+ (\\frac{10}{11})(\\frac{1}{6})^{n}}\\right]\\\\
 \& \= 10\\log\_{10}\\frac{10^{\-6}}{1 \- 10^{\-6}} \+ 10\\log\_{10}\\left\[\\frac{(\\frac{99}{100})^n}{(\\frac{1}{11})(\\frac{1}{3})^{n} \+ (\\frac{10}{11})(\\frac{1}{6})^{n}}\\right]\\\\
 \&\= 10\\log\_{10}\\left\[\\frac{10^{\-6}(\\frac{99}{100})^n}{(1 \- 10^{\-6})(\\frac{1}{11})(\\frac{1}{3})^{n} \+ (1 \- 10^{\-6})(\\frac{10}{11})(\\frac{1}{6})^{n}}\\right]
\\end{aligned}
\\]我们发现，\\(e(C\\mid DX)\\)现在可以由\\(e(C\\mid DX) \= 10\\log\_{10}\\left\[\\frac{P(C\\mid DX)}{1 \- P(C\\mid DX)}\\right]\\)得到。


现在，假设在检测刚开始时我们的机器人是刚出厂的，除了知道一台机器可能生产好部件也可能生成坏部件之外，它没有其它关于机器的先验知识。此时，机器人没有理由对于一个特定区间\\(\\mathrm{d}f\\)分配比其它区间更高的概率。因此，我们让机器人分配均匀先验概率密度\\(g(f\\mid X)\=\\text{常数}\\)。为了使得\\(\\int\_{0}^1g(f\\mid X)\\mathrm{d}f\=1\\)，我们取\\(g(f\\mid X)\=1, 0\\leqslant f\\leqslant 1\\)。此时，式\\((8\)\\)中的积分就是著名的第一类欧拉积分（现在通常称为完全Beta函数），我们有：


\\\[g(f\\mid DX) \= \\frac{f^{b}(1 \- f)^{n \- b}}{\\int\_{0}^1 f^{b}(1 \- f)^{n\-b}\\mathrm{d}f}
\= \\frac{f^{b}(1 \- f)^{n \- b}}{\\Beta(b \+ 1, n \- b \+ 1\)} \= \\frac{(n \+ 1\)!}{b!(n \- b)!}f^{b}(1 \- f)^{n \- b} \\tag{9}
\\]
> **注** 数学中有两种类型的**欧拉积分（Euler intergral）**\[4]：
> 
> 
> 1. 第一类欧拉积分（Beta函数）：


\\\[\\Beta(x, y) \= \\int\_{0}^1 t^{x \- 1}(1\-t)^{y\-1}\\mathrm{d}t \= \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x \+y)}
\\]
> 2. 第二类欧拉积分（Gamma函数）：


\\\[\\Gamma(z) \= \\int\_{0}^{\\infty}t^{z\-1}e^{\-t}\\mathrm{d}t
\\]上述后验分布在\\((0\\leqslant f\\leqslant 1\)\\)中有一个峰，通过令\\(g^{\\prime}(f\\mid DX)\=0\\)可以得到这是在\\(f \= \\hat{f}\=\\frac{b}{n}\\)处。其物理意义是观察到的坏部件比例或相对频率。为了寻找峰的尖锐程度，我们想对该函数进行进一步分析，由于该函数包括几个因子的累乘，我们对其进行取对数，得到：


\\\[\\mathcal{L}(f) \\equiv \\ln g(f\\mid DX) \= b\\ln f \+ (n \- b)\\ln (1 \- f) \+ \\text{常数}
\\]然后在\\(\\hat{f}\\)处对\\(\\mathcal{L(f)}\\)做Tayler展开：


\\\[\\begin{aligned}
\\mathcal{L}(f) \&\= \\mathcal{L}(\\hat{f}) \+ \\underbrace{\\mathcal{L}^{\\prime}(\\hat{f})(f \- \\hat{f})}\_{0} \+ \\frac{\\mathcal{L}^{\\prime\\prime}(\\hat{f})}{2!}(f \- \\hat{f})^2 \+ o\\left((f \- \\hat{f})^2\\right)\\\\
\&\= \\mathcal{L}(\\hat{f}) \- \\frac{(f \- \\hat{f})^2}{2\\sigma^2} \+ o\\left((f \- \\hat{f})^2\\right)
\\end{aligned}
\\]其中\\(\\sigma^2 \\equiv \\frac{\\hat{f}(1 \- \\hat{f})}{N}\\)（这里需要注意\\(\\mathcal{L}^{\\prime\\prime}(f)\=\\frac{\-nf^2 \+ 2bf \- b}{f^2(1 \- f)^2}, \\mathcal{L}^{\\prime\\prime}(\\hat{f}) \= \\frac{\-n\\frac{b^2}{n^2} \+ 2b\\frac{b}{n} \- b}{\\hat{f}^2(1 \- \\hat{f})^2}\=\\frac{b(\\frac{b}{n} \- 1\)}{\\hat{f}^2(1 \- \\hat{f})^2}\=\\frac{\-b(1 \- \\hat{f})}{\\hat{f}^2 (1 \- \\hat{f})^2}\=\-\\frac{b}{\\hat{f}^2(1 \- \\hat{f})}\=\-\\frac{b}{\\frac{b}{n}\\hat{f}(1 \- \\hat{f})}\=\-\\frac{n}{\\hat{f}(1 \- \\hat{f})}\\)）。


对于这个近似值，我们就得到了式\\((9\)\\)的近似分布：


\\\[g(f\\mid DX) \\approx K \\exp\\left\\{\-\\frac{(f \- \\hat{f})^2}{2\\sigma^2}\\right\\} \\tag{10}
\\]该分布称为**高斯分布（Gaussian distribution）**（或称**正态分布（normal distribution）**）。其中\\(K\=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\)是归一化常数，用于保证\\(\\int\_0^1 g(f\\mid DX)\=1\\)。实际上，只要\\(b\\gg 1\\)且\\((n \- b)\\gg 1\\)，这是在整个区间\\((0 \< f \< 1\)\\)中对式\\((9\)\\)的一个很好的逼近。



> **注** 关于二项分布的正态逼近，有棣莫弗\-拉普拉斯（de Moivre\-Laplace）中心极限定理对其进行刻画。设\\(n\\)重伯努利试验中，事件\\(A\\)在每次试验中出现的概率为\\(p\\)（\\(0），记\\(S\_n\\)为\\(n\\)次试验中事件\\(A\\)出现的次数，则当\\(n\\rightarrow \\infty\\)时，有\\(\\frac{S\_n}{n} \\rightarrow \\mathcal{N}(p, \\sqrt{\\frac{pq}{n}})\\)（依分布）。这里的\\(\\frac{S\_n}{n}\\)对应我们前面提到的\\(\\frac{b}{n}\\)，\\(p\\)对应我们前面提到的\\(\\hat{f}\=\\frac{b}{n}\\)，\\(q\\)对应我们前面提到的\\(1 \- \\hat{f}\\)。


因此，在\\(n\\)次试验中观察到\\(b\\)个坏部件后，\\(f\\)的最概然值（the most likely value）是观察到的坏部件的比例，这合理地描述了机器人关于\\(f\\)的知识状态。考虑\\(f\\)的准确性，这个估计使得\\(\\hat{f}\\pm\\sigma\\)很可能包含真实值。参数\\(\\sigma\\)称为PDF\\((10\)\\)的**标准差（standard deviation）**，\\(\\sigma^2\\)称为PDF\\((10\)\\)的**方差（variance）**。更准确地说，根据式\\((10\)\\)进行分析，机器人分配概率如下：


\\\[\\begin{aligned}
f\\text{的真实值}\&\\text{包含在 }\\hat{f}\\pm 0\.68\\sigma \\text{ 中的概率为 } 50\\%；\\\\
\&\\text{包含在 }\\hat{f}\\pm 1\.65\\sigma \\text{ 中的概率为 } 90\\%；\\\\
\&\\text{包含在 }\\hat{f}\\pm 2\.57\\sigma \\text{ 中的概率为 } 99\\%；
\\end{aligned}
\\]随着测试次数\\(n\\)的增加，这些区间会根据\\(\\sigma^2\=\\frac{\\hat{f}(1 \- \\hat{f})}{n}\\)，正比于\\(\\frac{1}{\\sqrt{n}}\\)按比例缩小。



> **注** 这里可以想到质量控制里用的较多的3 sigma法则（也被称为68\-95\-99\.7法则）\[5]，也即对于服从正态分布\\(\\mathcal{N}(\\mu, \\sigma^2\)\\)随机变量\\(X\\)，其观测值包含在\\(\\mu\\pm \\sigma\\)中的概率为\\(68\.3\\%\\)；包含在\\(\\mu\\pm 2\\sigma\\)中的概率为\\(95\.4\\%\\)；包含在\\(\\mu\\pm 3\\sigma\\)中的概率为\\(99\.7\\%\\)。


这样，我们看到机器人从对\\(f\\)的“无知”状态开始，随着从测试中积累信息，它对\\(f\\)的估计越来越确定，这与常识吻合。但是我们在这里需要强调，**\\(f\\)不会随时间变化，\\(\\sigma\\)不是\\(f\\)的真实属性而只是机器人表示其关于\\(f\\)的知识状态的概率分布的属性**。


# 6 简单假设与复合假设


到目前为止，我们考虑的假设（\\(A\\)、\\(B\\)、\\(C\\)、\\(H\_f\\)）指的是单个参数\\(f\=M/N\\)，即盒子中坏部件的未知比例，而且为\\(f\\)指定了一个明确定义的值（在\\(H\_f\\)中，它可以是\\(0\\leqslant f\\leqslant 1\\)中的任何数值）。这种假设称为**简单假设（simple hypothesis）**，因为如果定义了一个包含所有参数的参数空间\\(\\Omega\\)，这样的假设在\\(\\Omega\\)中由单个点表示。


然而，有时我们不需要检验\\(\\Omega\\)中的所有简单假设，只关心参数是位于某个子集\\(\\Omega\_1\\subseteq \\Omega\\)还是其补集\\(\\Omega\_2 \= \\Omega \- \\Omega\_1\\)中，而不关心该子集中\\(f\\)的特定值。我们称形如\\(H\\equiv f \\in \\Omega\_1\\)的假设为**复合假设（compound/composite hypothesis）**。我们是否可以直接处理复合假设，而不要求机器人检验\\(\\Omega\_1\\)中的每个简单假设呢？


事实上，在式\\((8\)\\)中，我们几乎完成了所有工作，接下来我们只需要再进行一次积分消除冗余参数即可。参数空间\\(\\Omega\\)由\\(\[0, 1]\\)中的所有\\(f\\)组成。假设若\\(f \> 0\.1\\)，我们需要采取一些措施（如关闭并重新调整机器）；若\\(f \\leqslant 0\.1\\)，则应该让机器继续运行。那么我们定义\\(\\Omega\_1\\)为\\(\[0\.1, 1]\\)中的所有\\(f\\),令复合假设\\(H\\equiv f\\in \\Omega\_1\\)。由于\\(f\\)的实际值无关紧要，\\(f\\)现在称为**冗余参数（nuisance parameter）**，我们想消去它。通过对冗余参数\\(f\\)求积分，可以将其从式\\((8\)\\)中消去：


\\\[P(\\Omega\_1\\mid DX) \= \\frac{\\int\_{\\Omega\_1}f^{b} (1\-f)^{n\-b}g(f\\mid X)}{\\int\_{\\Omega}f^{b} (1 \- f)^{n \- b}g(f\\mid X)\\mathrm{d}f}
\\]在\\(f\\)是均匀先验PDF的情况下，结果是不完全Beta函数：\\(f\\)在任何指定区间\\((a\_1 \< f \< a\_2\)\\)中的后验概率为


\\\[P(a\_1 \< f \< a\_2\\mid DX) \= \\frac{(N \+ 1\)!}{n!(N \- n)!}\\int\_{a\_1}^{a\_2}f^b(1 \- f)^{n \- b}\\mathrm{d}f
\\]计算机能够轻松计算这种形式的式子。


# 参考


* \[1] Jaynes E T. Probability theory: The logic of science\[M]. Cambridge university press, 2003\.
* \[2] 杰恩斯. 廖海仁译. 概率论沉思录\[M]. 人民邮电出版社, 2024\.
* \[3] Kant I, Meiklejohn J M D, Abbott T K, et al. Critique of pure reason\[M]. London: JM Dent, 1934\.
* \[4] [《维基百科：欧拉积分》](https://github.com)
* \[5] [《维基百科：68–95–99\.7法则》](https://github.com)


